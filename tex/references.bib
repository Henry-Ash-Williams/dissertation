@online{barnett2021chrome,
  author = {Daly Barnett},
  title  = {Chrome Users Beware: Manifest V3 is Deceitful and Threatening},
  year   = {2021},
  url    = {https://www.eff.org/deeplinks/2021/12/chrome-users-beware-manifest-v3-deceitful-and-threatening},
  urldate   = {Accessed 2024-04-17}
}

@article{barzvi2020eyetrackingdigialreading,
  author  = {Bar-Zvi Shaked, Karin and Shamir, Adina and Vakil, Eli},
  journal = {Reading and Writing},
  number  = {8},
  title   = {An eye tracking study of digital text reading: a comparison between poor and typical readers},
  volume  = {33},
  year    = {2020},
  pages   = {1925 -- 1944},
  doi     = {10.1007/s11145-020-10021-9},
  url     = {https://doi.org/10.1007/s11145-020-10021-9}
}

@misc{bcs2022coc,
  author  = {BCS, The Chartered Institute for IT},
  title   = {Code of Conduct for BCS Members},
  version = {8},
  url     = {https://www.bcs.org/media/2211/bcs-code-of-conduct.pdf},
  urldate = {2023-11-12},
  year    = {2022}
}

@techreport{bengtsson2020manifest,
  author      = {Peter Bengtsson},
  institution = {Mozilla Group},
  title       = {manifest.json},
  year        = {2020},
  note        = {Accessed 2020-04-17},
  url         = {https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/manifest.json}
}

% Gazecapture
@article{cheng2021survey,
  title   = {Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark},
  author  = {Yihua Cheng and Haofei Wang and Yiwei Bao and Feng Lu},
  journal = {arXiv preprint arXiv:2104.12668},
  year    = {2021}
}

@article{chutorian2009head,
  author   = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Head Pose Estimation in Computer Vision: A Survey},
  year     = {2009},
  volume   = {31},
  number   = {4},
  pages    = {607-626},
  keywords = {Computer vision;Head;Humans;Cameras;Face detection;Focusing;Neck;Eyes;Face recognition;Evolution (biology);Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology;Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology},
  doi      = {10.1109/TPAMI.2008.106}
}

@misc{finlay2015metamask,
  author       = {Dan Finlay},
  title        = {Metamask},
  year         = {2015},
  howpublished = {Github Repository},
  note         = {Accessed 2024-04-16},
  url          = {https://github.com/MetaMask/metamask-extension}
}

@book{frisbe2022building,
  title     = {Building Browser Extensions: Create Modern Extensions for Chrome, Safari, Firefox, and Edge},
  author    = {Matt Frisbie},
  publisher = {Apress},
  isbn      = {148428724X,9781484287248},
  year      = {2022},
  edition   = {1},
  url       = {http://gen.lib.rus.ec/book/index.php?md5=7E63E6249A822FB3B8B5F6D63706F6E2}
}

% ETH-XGaze
@misc{hill2014ublock,
  author       = {Raymond Hill},
  title        = {uBlock Origin},
  year         = {2014},
  howpublished = {Github Repository},
  note         = {Accessed 2024-04-16},
  url          = {https://github.com/gorhill/uBlock}
} 

@misc{huang2016tabletgaze,
  title         = {TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile Tablets},
  author        = {Qiong Huang and Ashok Veeraraghavan and Ashutosh Sabharwal},
  year          = {2016},
  eprint        = {1508.01244},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}


@article{kar2017review,
  author  = {Kar, Anuradha and Corcoran, Peter},
  journal = {IEEE Access},
  title   = {A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms},
  year    = {2017},
  volume  = {5},
  number  = {},
  pages   = {16495-16519},
  doi     = {10.1109/ACCESS.2017.2735633}
}

@article{kassner2014pupil,
  author     = {Moritz Kassner and
                William Patera and
                Andreas Bulling},
  title      = {Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile
                Gaze-based Interaction},
  journal    = {CoRR},
  volume     = {abs/1405.0006},
  year       = {2014},
  url        = {http://arxiv.org/abs/1405.0006},
  eprinttype = {arXiv},
  eprint     = {1405.0006},
  timestamp  = {Mon, 13 Aug 2018 16:47:48 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/KassnerPB14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{king2009dlib,
  author  = {Davis E. King},
  title   = {Dlib-ml: A Machine Learning Toolkit},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  pages   = {1755-1758}
}

@misc{king2015models,
  title        = {dlib-models},
  author       = {Davis E King},
  year         = {2015},
  howpublished = {Github Repository},
  url          = {https://github.com/davisking/dlib-models},
  note         = {Accessed 2024-04-22}
}

@misc{koehrsen2018transfer,
  title        = {Transfer Learning with Convolutional Neural Networks in PyTorch},
  author       = {Will Koehrsen},
  howpublished = {Blog Post},
  url          = {https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce},
  note         = {Accessed 2024-04-22},
  organization = {Towards Data Science},
  year         = {2018}
}

@misc{krafka2016eye,
  title         = {Eye Tracking for Everyone},
  author        = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
  year          = {2016},
  eprint        = {1606.05814},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{lastpass,
  author       = {LastPass},
  title        = {LastPass Password Manager},
  year         = {2024},
  howpublished = {Google Chrome Extension Store},
  note         = {Accessed 2024-04-16},
  url          = {https://chromewebstore.google.com/detail/lastpass-free-password-ma/hdokiejnpimakedhajhdlcegeplioahd}
}

@techreport{li2020manifest,
  author      = {David Li},
  institution = {Google LLC},
  title       = {Manifest V3 now available on M88 Beta},
  year        = {2020},
  note        = {Accessed 2020-04-17},
  url         = {https://blog.chromium.org/2020/12/manifest-v3-now-available-on-m88-beta.html}
}
@article{liu2022in,
  title    = {In the eye of the beholder: A survey of gaze tracking techniques},
  journal  = {Pattern Recognition},
  volume   = {132},
  pages    = {108944},
  year     = {2022},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2022.108944},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320322004241},
  author   = {Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin},
  keywords = {Gaze estimation, eye features, appearance-based, personal calibration, head motion},
  abstract = {Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.}
}

@misc{lsdsoftware2024read,
  author       = {John Huynh and Hai Phan},
  title        = {Read Aloud: A Text to Speech Voice Reader},
  year         = {2024},
  howpublished = {Google Chrome Extension Store},
  note         = {Accessed 2024-04-16},
  url          = {https://chromewebstore.google.com/detail/read-aloud-a-text-to-spee/hdhinadidafjejdhmfkjgnolgimiaplp}
}

@misc{mturk,
  author  = {Amazon},
  title   = {Mechanical Turk},
  year    = {2005},
  url     = {https://www.mturk.com/},
  urldate = {2023-11-14}
}


@inproceedings{ohno2002freegaze,
  author    = {Ohno, Takehiko and Mukawa, Naoki and Yoshikawa, Atsushi},
  title     = {FreeGaze: A Gaze Tracking System for Everyday Gaze Interaction},
  year      = {2002},
  isbn      = {1581134673},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/507072.507098},
  doi       = {10.1145/507072.507098},
  abstract  = {In this paper we introduce a novel gaze tracking system called FreeGaze, which is designed for the use of everyday gaze interaction. Among various possible applications of gaze tracking system, Human-Computer Interaction (HCI) is one of the most promising elds. However, existing systems require complicated and burden-some calibration and are not robust to the measurement variations. To solve these problems, we introduce a geometric eyeball model and sophisticated image processing. Unlike existing systems, our system needs only two points for each individual calibration. When the personalization nishes, our system needs no more calibration before each measurement session. Evaluation tests show that the system is accurate and applicable to everyday use for the applications.},
  booktitle = {Proceedings of the 2002 Symposium on Eye Tracking Research \& Applications},
  pages     = {125,132},
  numpages  = {8},
  keywords  = {gaze interaction, the eyeball model, FreeGaze, Gaze tracing system},
  location  = {New Orleans, Louisiana},
  series    = {ETRA '02}
}

@misc{palmero2018recurrent,
  title         = {Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues},
  author        = {Cristina Palmero and Javier Selva and Mohammad Ali Bagheri and Sergio Escalera},
  year          = {2018},
  eprint        = {1805.03064},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{pathirana2022eye,
  title    = {Eye gaze estimation: A survey on deep learning-based approaches},
  journal  = {Expert Systems with Applications},
  volume   = {199},
  pages    = {116894},
  year     = {2022},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2022.116894},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417422003347},
  author   = {Primesh Pathirana and Shashimal Senarath and Dulani Meedeniya and Sampath Jayarathna},
  keywords = {Computer vision, Gaze estimation, Deep learning, Eye tracking},
  abstract = {Human gaze estimation plays a major role in many applications in human–computer interaction and computer vision by identifying the users’ point-of-interest. Revolutionary developments of deep learning have captured significant attention in gaze estimation literature. Gaze estimation techniques have progressed from single-user constrained environments to multi-user unconstrained environments with the applicability of deep learning techniques in complex unconstrained environments with extensive variations. This paper presents a comprehensive survey of the single-user and multi-user gaze estimation approaches with deep learning. State-of-the-art approaches are analyzed based on deep learning model architectures, coordinate systems, environmental constraints, datasets and performance evaluation metrics. A key outcome from this survey realizes the limitations, challenges and future directions of multi-user gaze estimation techniques. Furthermore, this paper serves as a reference point and a guideline for future multi-user gaze estimation research.}
}

@misc{plasmo,
  author       = {Plasmo},
  howpublished = {Github},
  title        = {Plasmo Browser Extension Framework},
  url          = {https://www.plasmo.com/},
  urldate      = {2023-11-29},
  year         = {2022}
}
@article{rakhmatulin2020deep,
  title    = {Deep Neural Networks for Low-Cost Eye Tracking},
  journal  = {Procedia Computer Science},
  volume   = {176},
  pages    = {685-694},
  year     = {2020},
  note     = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 24th International Conference KES2020},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2020.09.041},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050920319360},
  author   = {Ildar Rakhmatulin and Andrew T. Duchowski},
  keywords = {eye tracking, deep learning in eye tracking, yolov3 in eye tracking, deep learning in gaze tracking},
  abstract = {The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center).}
}

@article{seonwook2019fewshot,
  author     = {Seonwook Park and
                Shalini De Mello and
                Pavlo Molchanov and
                Umar Iqbal and
                Otmar Hilliges and
                Jan Kautz},
  title      = {Few-shot Adaptive Gaze Estimation},
  journal    = {CoRR},
  volume     = {abs/1905.01941},
  year       = {2019},
  url        = {http://arxiv.org/abs/1905.01941},
  eprinttype = {arXiv},
  eprint     = {1905.01941},
  timestamp  = {Tue, 11 Jul 2023 08:22:03 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-01941.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{statcounter2024browser,
  title        = {Browser Market Share Worldwide},
  howpublished = {\url{https://web.archive.org/web/20240410052047/https://gs.statcounter.com/}},
  note         = {Accessed: 2024-04-10},
  author       = {StatCounter},
  year         = {2024}
}

@misc{tafasca2023sharingan,
  title         = {Sharingan: A Transformer-based Architecture for Gaze Following},
  author        = {Samy Tafasca and Anshul Gupta and Jean-Marc Odobez},
  year          = {2023},
  eprint        = {2310.00816},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{tobiiprofusion,
  author = {Tobii LTD},
  title  = {Tobii Pro Fusion},
  year   = 2019,
  url    = {https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion},
  note   = {Accessed on 2023-11-03}
}

@article{valliappan2020accelerating,
  author   = {Valliappan, Nachiappan
              and Dai, Na
              and Steinberg, Ethan
              and He, Junfeng
              and Rogers, Kantwon
              and Ramachandran, Venky
              and Xu, Pingmei
              and Shojaeizadeh, Mina
              and Guo, Li
              and Kohlhoff, Kai
              and Navalpakkam, Vidhya},
  title    = {Accelerating eye movement research via accurate and affordable smartphone eye tracking},
  journal  = {Nature Communications},
  year     = {2020},
  month    = {Sep},
  day      = {11},
  volume   = {11},
  number   = {1},
  pages    = {4553},
  abstract = {Eye tracking has been widely used for decades in vision research, language and usability. However, most prior research has focused on large desktop displays using specialized eye trackers that are expensive and cannot scale. Little is known about eye movement behavior on phones, despite their pervasiveness and large amount of time spent. We leverage machine learning to demonstrate accurate smartphone-based eye tracking without any additional hardware. We show that the accuracy of our method is comparable to state-of-the-art mobile eye trackers that are 100x more expensive. Using data from over 100 opted-in users, we replicate key findings from previous eye movement research on oculomotor tasks and saliency analyses during natural image viewing. In addition, we demonstrate the utility of smartphone-based gaze for detecting reading comprehension difficulty. Our results show the potential for scaling eye movement research by orders-of-magnitude to thousands of participants (with explicit consent), enabling advances in vision research, accessibility and healthcare.},
  issn     = {2041-1723},
  doi      = {10.1038/s41467-020-18360-5},
  url      = {https://doi.org/10.1038/s41467-020-18360-5}
}

@techreport{w3c2018accessible,
  author      = {Joanmarie Diggs and James Nurthen and Michael Cooper and Carolyn MacLeod and Shane McCarron and Richard Schwerdtfeger and James Craig},
  institution = {World Wide Web Consortium},
  title       = {Accessible Rich Internet Applications},
  year        = {2018}
}

@article{wu2024eg-net,
  title    = {EG-Net: Appearance-based eye gaze estimation using an efficient gaze network with attention mechanism},
  journal  = {Expert Systems with Applications},
  volume   = {238},
  pages    = {122363},
  year     = {2024},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2023.122363},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423028658},
  author   = {Xinmei Wu and Lin Li and Haihong Zhu and Gang Zhou and Linfeng Li and Fei Su and Shen He and Yanggang Wang and Xue Long},
  keywords = {Gaze estimation, Appearance-based method, EG-Net, Attention mechanism, Compound model scaling},
  abstract = {Gaze estimation, which has a wide range of applications in many scenarios, is a challenging task due to various unconstrained conditions. As information from both full-face and eye images is instrumental in improving gaze estimation, many multiregion gaze estimation models have been proposed in recent studies. However, most of them simply use the same regression method on both eye and face images, overlooking that the eye region may contribute more fine-grained features than the full-face region, and the variation in the left and right eyes of an individual caused by head pose, illumination, and partially occluded eye may lead to inconsistent estimations. To address these issues, we propose an appearance-based end-to-end learning network architecture with an attention mechanism, named efficient gaze network (EG-Net), which employs a two-branch network for gaze estimation. Specifically, a base CNN is utilized for full-face images, while an efficient eye network (EE-Net), which is scaled up from the base CNN, is used for left- and right-eye images. EE-Net uniformly scales up the depth, width and resolution of the base CNN with a set of constant coefficients for eye feature extraction and adaptively weights the left- and right-eye images via an attention network according to its “image quality”. Finally, features from the full-face image, two individual eye images and head pose vectors are fused to regress the eye gaze vectors. We evaluate our approach on 3 public datasets, the proposed EG-Net model achieves much better performance. In particular, our EG-Net-v4 model outperforms state-of-the-art approaches on the MPIIFaceGaze dataset, with prediction errors of 2.41 cm and 2.76 degrees in 2D and 3D gaze estimation, respectively. It also yields a performance improvement to 1.58 cm on GazeCapture and 4.55 degrees on EyeDIAP dataset, with 23.4 % and 14.2 % improvement over prior arts on the two datasets respectively. The code related to this project is open-source and available at https://github.com/wuxinmei/EE_Net.git.}
}

@article{yiuming2015eyegazetracking,
  author  = {Cheung, Yiu-ming and Peng, Qinmu},
  journal = {IEEE Transactions on Human-Machine Systems},
  title   = {Eye Gaze Tracking With a Web Camera in a Desktop Environment},
  year    = {2015},
  volume  = {45},
  number  = {4},
  pages   = {419-430},
  doi     = {10.1109/THMS.2015.2400442}
}

@article{young1975survey,
  author   = {Young, Laurence R.
              and Sheena, David},
  title    = {Survey of eye movement recording methods},
  journal  = {Behavior Research Methods {\&} Instrumentation},
  year     = {1975},
  month    = {Sep},
  day      = {01},
  volume   = {7},
  number   = {5},
  pages    = {397-429},
  abstract = {This paper reviews most of the known techniques for measuring eye movements, explaining their principle of operation and their primary advantages and disadvantages. The five sections of the paper cover the following topics: (1) types of eye movement, (2) characteristics of the eye which lend themselves to measurement and the principal approaches to the measurement of eye movement, (3) practical methods of measurement with especial attention to the new techniques, (4) general considerations guiding a selection of method, and (5) summarizing of the major findings in a concise table.},
  issn     = {1554-3528},
  doi      = {10.3758/BF03201553},
  url      = {https://doi.org/10.3758/BF03201553}
}
@inproceedings{yu2019deep,
  author    = {Yu, Yu and Liu, Gang and Odobez, Jean-Marc},
  title     = {Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model},
  year      = {2019},
  isbn      = {978-3-030-11011-6},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-11012-3_35},
  doi       = {10.1007/978-3-030-11012-3_35},
  abstract  = {As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones.},
  booktitle = {Computer Vision - ECCV 2018 Workshops: Munich, Germany, September 8-14, 2018, Proceedings, Part II},
  pages     = {456-474},
  numpages  = {19},
  location  = {Munich, Germany}
}

@article{zaho2024gazeswin,
  author         = {Zhao, Ruijie and Wang, Yuhuan and Luo, Sihui and Shou, Suyao and Tang, Pinyan},
  title          = {Gaze-Swin: Enhancing Gaze Estimation with a Hybrid CNN-Transformer Network and Dropkey Mechanism},
  journal        = {Electronics},
  volume         = {13},
  year           = {2024},
  number         = {2},
  article-number = {328},
  url            = {https://www.mdpi.com/2079-9292/13/2/328},
  issn           = {2079-9292},
  abstract       = {Gaze estimation, which seeks to reveal where a person is looking, provides a crucial clue for understanding human intentions and behaviors. Recently, Visual Transformer has achieved promising results in gaze estimation. However, dividing facial images into patches compromises the integrity of the image structure, which limits the inference performance. To tackle this challenge, we present Gaze-Swin, an end-to-end gaze estimation model formed with a dual-branch CNN-Transformer architecture. In Gaze-Swin, we adopt the Swin Transformer as the backbone network due to its effectiveness in handling long-range dependencies and extracting global features. Additionally, we incorporate a convolutional neural network as an auxiliary branch to capture local facial features and intricate texture details. To further enhance robustness and address overfitting issues in gaze estimation, we replace the original self-attention in the Transformer branch with Dropkey Assisted Attention (DA-Attention). In particular, this DA-Attention treats keys in the Transformer block as Dropout units and employs a decay Dropout rate schedule to preserve crucial gaze representations in deeper layers. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of our method in comparison to the state of the art.},
  doi            = {10.3390/electronics13020328}
}

@inproceedings{zhang15cvpr,
  author    = {Xucong Zhang and Yusuke Sugano and Mario Fritz and Bulling, Andreas},
  title     = {Appearance-based Gaze Estimation in the Wild},
  booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  pages     = {4511-4520}
}

@article{zhang2019mpii,
  title   = {MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation},
  author  = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  year    = {2019},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  doi     = {10.1109/TPAMI.2017.2778103},
  pages   = {162-175},
  volume  = {41},
  number  = {1}
}

@inproceedings{zhang2020ethxgaze,
  author    = {Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and Siyu Tang and Otmar Hilliges},
  title     = {ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation},
  year      = {2020},
  booktitle = {European Conference on Computer Vision (ECCV)}
}
@inproceedings{zhu2006nonlinear,
  title        = {Nonlinear eye gaze mapping function estimation via support vector regression},
  author       = {Zhu, Zhiwei and Ji, Qiang and Bennett, Kristin P},
  booktitle    = {18th International Conference on Pattern Recognition (ICPR'06)},
  volume       = {1},
  pages        = {1132--1135},
  year         = {2006},
  organization = {IEEE}
}

@INPROCEEDINGS{niyogi1996example,
  author={Niyogi, S. and Freeman, W.T.},
  booktitle={Proceedings of the Second International Conference on Automatic Face and Gesture Recognition}, 
  title={Example-based head tracking}, 
  year={1996},
  volume={},
  number={},
  pages={374-378},
  keywords={Head;Monitoring;Humans;Prototypes;Workstations;Road safety;Vector quantization;Neural networks;Computer science;Indexing},
  doi={10.1109/AFGR.1996.557294}}

@misc{klenov2015benchmark,
  title = {Async Python Web Frameworks Comparison},
  author = {Kirill Klenov}, 
  year = {2015}, 
  url = {https://github.com/klen/py-frameworks-bench},
  note = {Accessed 2024-04-23},
  howpublished = {Github Repository},
}

@misc{prevato2018blacksheep,
title = {BlackSheep},
author = {Roberto Prevato},
year = {2018},
url = {https://github.com/Neoteroi/BlackSheep},
howpublished = {Github Repository},
note = {Accessed 2024-04-23},

}
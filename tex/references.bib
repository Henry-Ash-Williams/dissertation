@article{akinyelu2020convolutional,
  author   = {Akinyelu, Andronicus A. and Blignaut, Pieter},
  journal  = {IEEE Access},
  title    = {Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey},
  year     = {2020},
  volume   = {8},
  number   = {},
  pages    = {142581-142605},
  keywords = {Estimation;Solid modeling;Visualization;Machine learning;Computational modeling;Feature extraction;Gaze tracking;Convolutional neural network;deep learning;eye tracking;eye movements;gaze estimation;computer vision;region of interest},
  doi      = {10.1109/ACCESS.2020.3013540}
}

@misc{ali2020benchmark,
  author       = {Rousan Ali},
  title        = {Benchmarking Rust Web Frameworks: hyper vs gotham vs actix-web vs warp vs rocket},
  howpublished = {Github Repository},
  year         = {2020},
  url          = {https://github.com/rousan/rust-web-frameworks-benchmark},
  note         = {Accessed 2024-04-30}
}

@misc{balim2023efe,
  title         = {EFE: End-to-end Frame-to-Gaze Estimation},
  author        = {Haldun Balim and Seonwook Park and Xi Wang and Xucong Zhang and Otmar Hilliges},
  year          = {2023},
  eprint        = {2305.05526},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{bao2021adaptive,
  author     = {Yiwei Bao and
                Yihua Cheng and
                Yunfei Liu and
                Feng Lu},
  title      = {Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets},
  journal    = {CoRR},
  volume     = {abs/2103.11119},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.11119},
  eprinttype = {arXiv},
  eprint     = {2103.11119},
  timestamp  = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-11119.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

% Gazecapture
@online{barnett2021chrome,
  author  = {Daly Barnett},
  title   = {Chrome Users Beware: Manifest V3 is Deceitful and Threatening},
  year    = {2021},
  url     = {https://www.eff.org/deeplinks/2021/12/chrome-users-beware-manifest-v3-deceitful-and-threatening},
  urldate = {Accessed 2024-04-17}
}

@article{barzvi2020eyetrackingdigialreading,
  author  = {Bar-Zvi Shaked, Karin and Shamir, Adina and Vakil, Eli},
  journal = {Reading and Writing},
  number  = {8},
  title   = {An eye tracking study of digital text reading: a comparison between poor and typical readers},
  volume  = {33},
  year    = {2020},
  pages   = {1925 -- 1944},
  doi     = {10.1007/s11145-020-10021-9},
  url     = {https://doi.org/10.1007/s11145-020-10021-9}
}

@misc{bcs2022coc,
  author  = {BCS, The Chartered Institute for IT},
  title   = {Code of Conduct for BCS Members},
  version = {8},
  url     = {https://www.bcs.org/media/2211/bcs-code-of-conduct.pdf},
  urldate = {2023-11-12},
  year    = {2022}
}

@techreport{bengtsson2020manifest,
  author      = {Peter Bengtsson},
  institution = {Mozilla Group},
  title       = {manifest.json},
  year        = {2020},
  note        = {Accessed 2020-04-17},
  url         = {https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/manifest.json}
}

% ETH-XGaze
@article{cheng2021survey,
  title   = {Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark},
  author  = {Yihua Cheng and Haofei Wang and Yiwei Bao and Feng Lu},
  journal = {arXiv preprint arXiv:2104.12668},
  year    = {2021}
} 

@article{chutorian2009head,
  author   = {Murphy-Chutorian, Erik and Trivedi, Mohan Manubhai},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Head Pose Estimation in Computer Vision: A Survey},
  year     = {2009},
  volume   = {31},
  number   = {4},
  pages    = {607-626},
  keywords = {Computer vision;Head;Humans;Cameras;Face detection;Focusing;Neck;Eyes;Face recognition;Evolution (biology);Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology;Introductory and Survey;Computer vision;Modeling and recovery of physical attributes;Human-centered computing;Vision I/O;Face and gesture recognition;Evaluation/methodology},
  doi      = {10.1109/TPAMI.2008.106}
}


@inproceedings{deng2009imagenet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {ImageNet: A large-scale hierarchical image database},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {248-255},
  keywords  = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi       = {10.1109/CVPR.2009.5206848}
}

@phdthesis{fielding2000phd,
  abstract  = {The World Wide Web has succeeded in large part because its software
               architecture has been designed to meet the needs of an Internet-scale
               distributed hypermedia system. The Web has been iteratively developed
               over the past ten years through a series of modifications to the
               standards that define its architecture. In order to identify those
               aspects of the Web that needed improvement and avoid undesirable
               modifications, a model for the modern Web architecture was needed
               to guide its design, definition, and deployment.
               
               Software architecture research investigates methods for determining
               how best to partition a system, how components identify and communicate
               with each other, how information is communicated, how elements of
               a system can evolve independently, and how all of the above can be
               described using formal and informal notations. My work is motivated
               by the desire to understand and evaluate the architectural design
               of network-based application software through principled use of architectural
               constraints, thereby obtaining the functional, performance, and social
               properties desired of an architecture. An architectural style is
               a named, coordinated set of architectural constraints.
               
               This dissertation defines a framework for understanding software architecture
               via architectural styles and demonstrates how styles can be used
               to guide the architectural design of network-based application software.
               A survey of architectural styles for network-based applications is
               used to classify styles according to the architectural properties
               they induce on an architecture for distributed hypermedia. I then
               introduce the Representational State Transfer (REST) architectural
               style and describe how REST has been used to guide the design and
               development of the architecture for the modern Web.
               
               REST emphasizes scalability of component interactions, generality
               of interfaces, independent deployment of components, and intermediary
               components to reduce interaction latency, enforce security, and encapsulate
               legacy systems. I describe the software engineering principles guiding
               REST and the interaction constraints chosen to retain those principles,
               contrasting them to the constraints of other architectural styles.
               Finally, I describe the lessons learned from applying REST to the
               design of the Hypertext Transfer Protocol and Uniform Resource Identifier
               standards, and from their subsequent deployment in Web client and
               server software.},
  added-at  = {2008-03-05T21:11:28.000+0100},
  author    = {Fielding, Roy Thomas},
  biburl    = {https://www.bibsonomy.org/bibtex/217b085721104f50d2f804bd1df197edc/gromgull},
  file      = {Site:2000/Fielding2000Phd.pdf:PDF},
  interhash = {092ba78050ae9da156348ac20fccbd12},
  intrahash = {17b085721104f50d2f804bd1df197edc},
  keywords  = {architecture rest},
  owner     = {flint},
  school    = {University of California, Irvine},
  timestamp = {2008-03-05T21:11:28.000+0100},
  title     = {{REST:} Architectural Styles and the Design of Network-based Software
               Architectures},
  type      = {Doctoral dissertation},
  url       = {http://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm},
  year      = 2000
}

@misc{finlay2015metamask,
  author       = {Dan Finlay},
  title        = {Metamask},
  year         = {2015},
  howpublished = {Github Repository},
  note         = {Accessed 2024-04-16},
  url          = {https://github.com/MetaMask/metamask-extension}
}

@book{frisbe2022building,
  title     = {Building Browser Extensions: Create Modern Extensions for Chrome, Safari, Firefox, and Edge},
  author    = {Matt Frisbie},
  publisher = {Apress},
  isbn      = {148428724X,9781484287248},
  year      = {2022},
  edition   = {1},
  url       = {http://gen.lib.rus.ec/book/index.php?md5=7E63E6249A822FB3B8B5F6D63706F6E2}
}

@misc{he2015deep,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{hill2014ublock,
  author       = {Raymond Hill},
  title        = {uBlock Origin},
  year         = {2014},
  howpublished = {Github Repository},
  note         = {Accessed 2024-04-16},
  url          = {https://github.com/gorhill/uBlock}
}

@misc{httparchive2024pageweight,
  title        = {Page Weight},
  year         = {2024},
  author       = {httparchive},
  howpublished = {Website},
  note         = {Accessed 2024-05-01},
  url          = {https://httparchive.org/reports/page-weight}
}

@misc{huang2016tabletgaze,
  title         = {TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile Tablets},
  author        = {Qiong Huang and Ashok Veeraraghavan and Ashutosh Sabharwal},
  year          = {2016},
  eprint        = {1508.01244},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{junfeng2019on,
  author    = {He, Junfeng and Pham, Khoi and Valliappan, Nachiappan and Xu, Pingmei and Roberts, Chase and Lagun, Dmitry and Navalpakkam, Vidhya},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  title     = {On-Device Few-Shot Personalization for Real-Time Gaze Estimation},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1149-1158},
  keywords  = {Estimation;Calibration;Computational modeling;Face;Two dimensional displays;Data models;Cameras;gaze;gaze tracking;few shot learning;on device learning;personalization},
  doi       = {10.1109/ICCVW.2019.00146}
}

@article{kar2017review,
  author  = {Kar, Anuradha and Corcoran, Peter},
  journal = {IEEE Access},
  title   = {A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms},
  year    = {2017},
  volume  = {5},
  number  = {},
  pages   = {16495-16519},
  doi     = {10.1109/ACCESS.2017.2735633}
}

@article{kassner2014pupil,
  author     = {Moritz Kassner and
                William Patera and
                Andreas Bulling},
  title      = {Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile
                Gaze-based Interaction},
  journal    = {CoRR},
  volume     = {abs/1405.0006},
  year       = {2014},
  url        = {http://arxiv.org/abs/1405.0006},
  eprinttype = {arXiv},
  eprint     = {1405.0006},
  timestamp  = {Mon, 13 Aug 2018 16:47:48 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/KassnerPB14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{king2009dlib,
  author  = {Davis E. King},
  title   = {Dlib-ml: A Machine Learning Toolkit},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  pages   = {1755-1758}
}

@misc{king2015models,
  title        = {dlib-models},
  author       = {Davis E King},
  year         = {2015},
  howpublished = {Github Repository},
  url          = {https://github.com/davisking/dlib-models},
  note         = {Accessed 2024-04-22}
}

@misc{klenov2015benchmark,
  title        = {Async Python Web Frameworks Comparison},
  author       = {Kirill Klenov},
  year         = {2015},
  url          = {https://github.com/klen/py-frameworks-bench},
  note         = {Accessed 2024-04-23},
  howpublished = {Github Repository}
}

@misc{koehrsen2018transfer,
  title        = {Transfer Learning with Convolutional Neural Networks in PyTorch},
  author       = {Will Koehrsen},
  howpublished = {Blog Post},
  url          = {https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce},
  note         = {Accessed 2024-04-22},
  organization = {Towards Data Science},
  year         = {2018}
}
@misc{krafka2016eye,
  title         = {Eye Tracking for Everyone},
  author        = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
  year          = {2016},
  eprint        = {1606.05814},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{krizhevsky2017imagenet,
  author     = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title      = {ImageNet classification with deep convolutional neural networks},
  year       = {2017},
  issue_date = {June 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {60},
  number     = {6},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3065386},
  doi        = {10.1145/3065386},
  abstract   = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  journal    = {Commun. ACM},
  month      = {may},
  pages      = {84–90},
  numpages   = {7}
}

@misc{lastpass,
  author       = {LastPass},
  title        = {LastPass Password Manager},
  year         = {2024},
  howpublished = {Google Chrome Extension Store},
  note         = {Accessed 2024-04-16},
  url          = {https://chromewebstore.google.com/detail/lastpass-free-password-ma/hdokiejnpimakedhajhdlcegeplioahd}
}

@techreport{li2020manifest,
  author      = {David Li},
  institution = {Google LLC},
  title       = {Manifest V3 now available on M88 Beta},
  year        = {2020},
  note        = {Accessed 2020-04-17},
  url         = {https://blog.chromium.org/2020/12/manifest-v3-now-available-on-m88-beta.html}
}

@article{liu2022in,
  title    = {In the eye of the beholder: A survey of gaze tracking techniques},
  journal  = {Pattern Recognition},
  volume   = {132},
  pages    = {108944},
  year     = {2022},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2022.108944},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320322004241},
  author   = {Jiahui Liu and Jiannan Chi and Huijie Yang and Xucheng Yin},
  keywords = {Gaze estimation, eye features, appearance-based, personal calibration, head motion},
  abstract = {Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.}
}

@misc{lsdsoftware2024read,
  author       = {John Huynh and Hai Phan},
  title        = {Read Aloud: A Text to Speech Voice Reader},
  year         = {2024},
  howpublished = {Google Chrome Extension Store},
  note         = {Accessed 2024-04-16},
  url          = {https://chromewebstore.google.com/detail/read-aloud-a-text-to-spee/hdhinadidafjejdhmfkjgnolgimiaplp}
}

@misc{mturk,
  author  = {Amazon},
  title   = {Mechanical Turk},
  year    = {2005},
  url     = {https://www.mturk.com/},
  urldate = {2023-11-14}
}

@article{mutalib2022student,
  title        = {Student Reading Skills Improvement Through Dyslexia Reading Procedures},
  volume       = {1},
  url          = {https://publication.seameosen.edu.my/index.php/diebook/article/view/328},
  abstractnote = {By using a window ruler, this best practice aims to improve dyslexic students' reading skills based on the reading module. Reading is a complex activity that involves both physical and mental activities. Reading activities are also linked to eye movements and visual acuity. Reading methods based on the Dyslexia Reading Procedure can help students improve their reading skills and reduce syllable, word, and sentence reading errors without skipping lines. This practice involves four Sakura dyslexic students from SMK Padang Temu Melaka (Padang Temu Secondary School) in a report and observation format. The use of this best practice implies that teachers can use the Dyslexia Reading Procedure during teaching sessions by using teaching aids. It is hoped that this best practice will help boost the confidence and motivation of special education students, particularly dyslexic students, as well as make the teaching and facilitation process (PdPc) more enjoyable to implement.&amp;lt;/p&amp;gt;},
  number       = {1},
  journal      = {Best Practices in Disability-Inclusive Education},
  author       = {Mazlinah binti Abd Mutalib},
  year         = {2022},
  month        = {Dec.}
}

@inproceedings{niyogi1996example,
  author    = {Niyogi, S. and Freeman, W.T.},
  booktitle = {Proceedings of the Second International Conference on Automatic Face and Gesture Recognition},
  title     = {Example-based head tracking},
  year      = {1996},
  volume    = {},
  number    = {},
  pages     = {374-378},
  keywords  = {Head;Monitoring;Humans;Prototypes;Workstations;Road safety;Vector quantization;Neural networks;Computer science;Indexing},
  doi       = {10.1109/AFGR.1996.557294}
}

@inproceedings{ohno2002freegaze,
  author    = {Ohno, Takehiko and Mukawa, Naoki and Yoshikawa, Atsushi},
  title     = {FreeGaze: A Gaze Tracking System for Everyday Gaze Interaction},
  year      = {2002},
  isbn      = {1581134673},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/507072.507098},
  doi       = {10.1145/507072.507098},
  abstract  = {In this paper we introduce a novel gaze tracking system called FreeGaze, which is designed for the use of everyday gaze interaction. Among various possible applications of gaze tracking system, Human-Computer Interaction (HCI) is one of the most promising elds. However, existing systems require complicated and burden-some calibration and are not robust to the measurement variations. To solve these problems, we introduce a geometric eyeball model and sophisticated image processing. Unlike existing systems, our system needs only two points for each individual calibration. When the personalization nishes, our system needs no more calibration before each measurement session. Evaluation tests show that the system is accurate and applicable to everyday use for the applications.},
  booktitle = {Proceedings of the 2002 Symposium on Eye Tracking Research \& Applications},
  pages     = {125,132},
  numpages  = {8},
  keywords  = {gaze interaction, the eyeball model, FreeGaze, Gaze tracing system},
  location  = {New Orleans, Louisiana},
  series    = {ETRA '02}
}
@misc{onnxruntime,
  title        = {ONNX Runtime},
  author       = {ONNX Runtime developers},
  year         = {2021},
  howpublished = {\url{https://onnxruntime.ai/}},
  note         = {Version: 1.17.3}
}

@misc{pagevisibility,
  title  = {Page Visibility API},
  author = {Mozilla MDN},
  url    = {https://developer.mozilla.org/en-US/docs/Web/API/Page_Visibility_API},
  year   = {2024},
  note   = {Accessed 2024-04-29}
}

@misc{palmero2018recurrent,
  title         = {Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues},
  author        = {Cristina Palmero and Javier Selva and Mohammad Ali Bagheri and Sergio Escalera},
  year          = {2018},
  eprint        = {1805.03064},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{pathirana2022eye,
  title    = {Eye gaze estimation: A survey on deep learning-based approaches},
  journal  = {Expert Systems with Applications},
  volume   = {199},
  pages    = {116894},
  year     = {2022},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2022.116894},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417422003347},
  author   = {Primesh Pathirana and Shashimal Senarath and Dulani Meedeniya and Sampath Jayarathna},
  keywords = {Computer vision, Gaze estimation, Deep learning, Eye tracking},
  abstract = {Human gaze estimation plays a major role in many applications in human–computer interaction and computer vision by identifying the users’ point-of-interest. Revolutionary developments of deep learning have captured significant attention in gaze estimation literature. Gaze estimation techniques have progressed from single-user constrained environments to multi-user unconstrained environments with the applicability of deep learning techniques in complex unconstrained environments with extensive variations. This paper presents a comprehensive survey of the single-user and multi-user gaze estimation approaches with deep learning. State-of-the-art approaches are analyzed based on deep learning model architectures, coordinate systems, environmental constraints, datasets and performance evaluation metrics. A key outcome from this survey realizes the limitations, challenges and future directions of multi-user gaze estimation techniques. Furthermore, this paper serves as a reference point and a guideline for future multi-user gaze estimation research.}
}

@misc{plasmo,
  author       = {Plasmo},
  howpublished = {Github},
  title        = {Plasmo Browser Extension Framework},
  url          = {https://www.plasmo.com/},
  urldate      = {2023-11-29},
  year         = {2022}
}
@misc{prevato2018blacksheep,
  title        = {BlackSheep},
  author       = {Roberto Prevato},
  year         = {2018},
  url          = {https://github.com/Neoteroi/BlackSheep},
  howpublished = {Github Repository},
  note         = {Accessed 2024-04-23}
}

@article{rakhmatulin2020deep,
  title    = {Deep Neural Networks for Low-Cost Eye Tracking},
  journal  = {Procedia Computer Science},
  volume   = {176},
  pages    = {685-694},
  year     = {2020},
  note     = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 24th International Conference KES2020},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2020.09.041},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050920319360},
  author   = {Ildar Rakhmatulin and Andrew T. Duchowski},
  keywords = {eye tracking, deep learning in eye tracking, yolov3 in eye tracking, deep learning in gaze tracking},
  abstract = {The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center).}
}

@article{seonwook2019fewshot,
  author     = {Seonwook Park and
                Shalini De Mello and
                Pavlo Molchanov and
                Umar Iqbal and
                Otmar Hilliges and
                Jan Kautz},
  title      = {Few-shot Adaptive Gaze Estimation},
  journal    = {CoRR},
  volume     = {abs/1905.01941},
  year       = {2019},
  url        = {http://arxiv.org/abs/1905.01941},
  eprinttype = {arXiv},
  eprint     = {1905.01941},
  timestamp  = {Tue, 11 Jul 2023 08:22:03 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-01941.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{statcounter2024browser,
  title        = {Browser Market Share Worldwide},
  howpublished = {\url{https://web.archive.org/web/20240410052047/https://gs.statcounter.com/}},
  note         = {Accessed: 2024-04-10},
  author       = {StatCounter},
  year         = {2024}
}

@misc{tafasca2023sharingan,
  title         = {Sharingan: A Transformer-based Architecture for Gaze Following},
  author        = {Samy Tafasca and Anshul Gupta and Jean-Marc Odobez},
  year          = {2023},
  eprint        = {2310.00816},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{tensorflow2015whitepaper,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Mart\'{i}n~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dandelion~Man\'{e} and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Vi\'{e}gas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
  year   = {2015}
}

@misc{tobiiprofusion,
  author = {Tobii LTD},
  title  = {Tobii Pro Fusion},
  year   = 2019,
  url    = {https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion},
  note   = {Accessed on 2023-11-03}
}

@article{valliappan2020accelerating,
  author   = {Valliappan, Nachiappan
              and Dai, Na
              and Steinberg, Ethan
              and He, Junfeng
              and Rogers, Kantwon
              and Ramachandran, Venky
              and Xu, Pingmei
              and Shojaeizadeh, Mina
              and Guo, Li
              and Kohlhoff, Kai
              and Navalpakkam, Vidhya},
  title    = {Accelerating eye movement research via accurate and affordable smartphone eye tracking},
  journal  = {Nature Communications},
  year     = {2020},
  month    = {Sep},
  day      = {11},
  volume   = {11},
  number   = {1},
  pages    = {4553},
  abstract = {Eye tracking has been widely used for decades in vision research, language and usability. However, most prior research has focused on large desktop displays using specialized eye trackers that are expensive and cannot scale. Little is known about eye movement behavior on phones, despite their pervasiveness and large amount of time spent. We leverage machine learning to demonstrate accurate smartphone-based eye tracking without any additional hardware. We show that the accuracy of our method is comparable to state-of-the-art mobile eye trackers that are 100x more expensive. Using data from over 100 opted-in users, we replicate key findings from previous eye movement research on oculomotor tasks and saliency analyses during natural image viewing. In addition, we demonstrate the utility of smartphone-based gaze for detecting reading comprehension difficulty. Our results show the potential for scaling eye movement research by orders-of-magnitude to thousands of participants (with explicit consent), enabling advances in vision research, accessibility and healthcare.},
  issn     = {2041-1723},
  doi      = {10.1038/s41467-020-18360-5},
  url      = {https://doi.org/10.1038/s41467-020-18360-5}
}

@techreport{w3c2018accessible,
  author      = {Joanmarie Diggs and James Nurthen and Michael Cooper and Carolyn MacLeod and Shane McCarron and Richard Schwerdtfeger and James Craig},
  institution = {World Wide Web Consortium},
  title       = {Accessible Rich Internet Applications},
  year        = {2018}
}

@article{wu2024eg-net,
  title    = {EG-Net: Appearance-based eye gaze estimation using an efficient gaze network with attention mechanism},
  journal  = {Expert Systems with Applications},
  volume   = {238},
  pages    = {122363},
  year     = {2024},
  issn     = {0957-4174},
  doi      = {https://doi.org/10.1016/j.eswa.2023.122363},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423028658},
  author   = {Xinmei Wu and Lin Li and Haihong Zhu and Gang Zhou and Linfeng Li and Fei Su and Shen He and Yanggang Wang and Xue Long},
  keywords = {Gaze estimation, Appearance-based method, EG-Net, Attention mechanism, Compound model scaling},
  abstract = {Gaze estimation, which has a wide range of applications in many scenarios, is a challenging task due to various unconstrained conditions. As information from both full-face and eye images is instrumental in improving gaze estimation, many multiregion gaze estimation models have been proposed in recent studies. However, most of them simply use the same regression method on both eye and face images, overlooking that the eye region may contribute more fine-grained features than the full-face region, and the variation in the left and right eyes of an individual caused by head pose, illumination, and partially occluded eye may lead to inconsistent estimations. To address these issues, we propose an appearance-based end-to-end learning network architecture with an attention mechanism, named efficient gaze network (EG-Net), which employs a two-branch network for gaze estimation. Specifically, a base CNN is utilized for full-face images, while an efficient eye network (EE-Net), which is scaled up from the base CNN, is used for left- and right-eye images. EE-Net uniformly scales up the depth, width and resolution of the base CNN with a set of constant coefficients for eye feature extraction and adaptively weights the left- and right-eye images via an attention network according to its “image quality”. Finally, features from the full-face image, two individual eye images and head pose vectors are fused to regress the eye gaze vectors. We evaluate our approach on 3 public datasets, the proposed EG-Net model achieves much better performance. In particular, our EG-Net-v4 model outperforms state-of-the-art approaches on the MPIIFaceGaze dataset, with prediction errors of 2.41 cm and 2.76 degrees in 2D and 3D gaze estimation, respectively. It also yields a performance improvement to 1.58 cm on GazeCapture and 4.55 degrees on EyeDIAP dataset, with 23.4 % and 14.2 % improvement over prior arts on the two datasets respectively. The code related to this project is open-source and available at https://github.com/wuxinmei/EE_Net.git.}
}

@article{yiuming2015eyegazetracking,
  author  = {Cheung, Yiu-ming and Peng, Qinmu},
  journal = {IEEE Transactions on Human-Machine Systems},
  title   = {Eye Gaze Tracking With a Web Camera in a Desktop Environment},
  year    = {2015},
  volume  = {45},
  number  = {4},
  pages   = {419-430},
  doi     = {10.1109/THMS.2015.2400442}
}

@article{young1975survey,
  author   = {Young, Laurence R.
              and Sheena, David},
  title    = {Survey of eye movement recording methods},
  journal  = {Behavior Research Methods {\&} Instrumentation},
  year     = {1975},
  month    = {Sep},
  day      = {01},
  volume   = {7},
  number   = {5},
  pages    = {397-429},
  abstract = {This paper reviews most of the known techniques for measuring eye movements, explaining their principle of operation and their primary advantages and disadvantages. The five sections of the paper cover the following topics: (1) types of eye movement, (2) characteristics of the eye which lend themselves to measurement and the principal approaches to the measurement of eye movement, (3) practical methods of measurement with especial attention to the new techniques, (4) general considerations guiding a selection of method, and (5) summarizing of the major findings in a concise table.},
  issn     = {1554-3528},
  doi      = {10.3758/BF03201553},
  url      = {https://doi.org/10.3758/BF03201553}
}

@inproceedings{yu2019deep,
  author    = {Yu, Yu and Liu, Gang and Odobez, Jean-Marc},
  title     = {Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model},
  year      = {2019},
  isbn      = {978-3-030-11011-6},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-11012-3_35},
  doi       = {10.1007/978-3-030-11012-3_35},
  abstract  = {As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones.},
  booktitle = {Computer Vision - ECCV 2018 Workshops: Munich, Germany, September 8-14, 2018, Proceedings, Part II},
  pages     = {456-474},
  numpages  = {19},
  location  = {Munich, Germany}
}

@inproceedings{yun2019moving,
  author    = {Ma, Yun and Xiang, Dongwei and Zheng, Shuyu and Tian, Deyu and Liu, Xuanzhe},
  title     = {Moving Deep Learning into Web Browser: How Far Can We Go?},
  year      = {2019},
  isbn      = {9781450366748},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3308558.3313639},
  doi       = {10.1145/3308558.3313639},
  abstract  = {Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been supported in browsers so far. Then we measure the performance of different frameworks when running different deep learning tasks. Finally, we dig out the performance gap between deep learning in browsers and on native platforms by comparing the performance of TensorFlow.js and TensorFlow in Python. Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers.},
  booktitle = {The World Wide Web Conference},
  pages     = {1234–1244},
  numpages  = {11},
  keywords  = {Deep learning, Measurement, Web applications, Web browser},
  location  = {San Francisco, CA, USA},
  series    = {WWW '19}
}
@article{zaho2024gazeswin,
  author         = {Zhao, Ruijie and Wang, Yuhuan and Luo, Sihui and Shou, Suyao and Tang, Pinyan},
  title          = {Gaze-Swin: Enhancing Gaze Estimation with a Hybrid CNN-Transformer Network and Dropkey Mechanism},
  journal        = {Electronics},
  volume         = {13},
  year           = {2024},
  number         = {2},
  article-number = {328},
  url            = {https://www.mdpi.com/2079-9292/13/2/328},
  issn           = {2079-9292},
  abstract       = {Gaze estimation, which seeks to reveal where a person is looking, provides a crucial clue for understanding human intentions and behaviors. Recently, Visual Transformer has achieved promising results in gaze estimation. However, dividing facial images into patches compromises the integrity of the image structure, which limits the inference performance. To tackle this challenge, we present Gaze-Swin, an end-to-end gaze estimation model formed with a dual-branch CNN-Transformer architecture. In Gaze-Swin, we adopt the Swin Transformer as the backbone network due to its effectiveness in handling long-range dependencies and extracting global features. Additionally, we incorporate a convolutional neural network as an auxiliary branch to capture local facial features and intricate texture details. To further enhance robustness and address overfitting issues in gaze estimation, we replace the original self-attention in the Transformer branch with Dropkey Assisted Attention (DA-Attention). In particular, this DA-Attention treats keys in the Transformer block as Dropout units and employs a decay Dropout rate schedule to preserve crucial gaze representations in deeper layers. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of our method in comparison to the state of the art.},
  doi            = {10.3390/electronics13020328}
}
@inproceedings{zhang15cvpr,
  author    = {Xucong Zhang and Yusuke Sugano and Mario Fritz and Bulling, Andreas},
  title     = {Appearance-based Gaze Estimation in the Wild},
  booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  pages     = {4511-4520}
}


@article{zhang2019mpii,
  title   = {MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation},
  author  = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  year    = {2019},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  doi     = {10.1109/TPAMI.2017.2778103},
  pages   = {162-175},
  volume  = {41},
  number  = {1}
}

@inproceedings{zhang2020ethxgaze,
  author    = {Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and Siyu Tang and Otmar Hilliges},
  title     = {ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation},
  year      = {2020},
  booktitle = {European Conference on Computer Vision (ECCV)}
}

@inproceedings{zhu2006nonlinear,
  title        = {Nonlinear eye gaze mapping function estimation via support vector regression},
  author       = {Zhu, Zhiwei and Ji, Qiang and Bennett, Kristin P},
  booktitle    = {18th International Conference on Pattern Recognition (ICPR'06)},
  volume       = {1},
  pages        = {1132--1135},
  year         = {2006},
  organization = {IEEE}
}
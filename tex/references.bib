% MPIIGaze
@article{cheng2021survey,
  title   = {Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark},
  author  = {Yihua Cheng and Haofei Wang and Yiwei Bao and Feng Lu},
  journal = {arXiv preprint arXiv:2104.12668},
  year    = {2021}
}

@book{frisbe2022building,
  title     = {Building Browser Extensions: Create Modern Extensions for Chrome, Safari, Firefox, and Edge},
  author    = {Matt Frisbie},
  publisher = {Apress},
  isbn      = {148428724X,9781484287248},
  year      = {2022},
  edition   = {1},
  url       = {http://gen.lib.rus.ec/book/index.php?md5=7E63E6249A822FB3B8B5F6D63706F6E2}
}

@article{kar2017review,
  author  = {Kar, Anuradha and Corcoran, Peter},
  journal = {IEEE Access},
  title   = {A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms},
  year    = {2017},
  volume  = {5},
  number  = {},
  pages   = {16495-16519},
  doi     = {10.1109/ACCESS.2017.2735633}
}

% Gazecapture
@misc{krafka2016eye,
  title         = {Eye Tracking for Everyone},
  author        = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
  year          = {2016},
  eprint        = {1606.05814},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{palmero2018recurrent,
  title         = {Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues},
  author        = {Cristina Palmero and Javier Selva and Mohammad Ali Bagheri and Sergio Escalera},
  year          = {2018},
  eprint        = {1805.03064},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@inproceedings{yu2019deep,
  author    = {Yu, Yu and Liu, Gang and Odobez, Jean-Marc},
  title     = {Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model},
  year      = {2019},
  isbn      = {978-3-030-11011-6},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-11012-3_35},
  doi       = {10.1007/978-3-030-11012-3_35},
  abstract  = {As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones.},
  booktitle = {Computer Vision - ECCV 2018 Workshops: Munich, Germany, September 8-14, 2018, Proceedings, Part II},
  pages     = {456-474},
  numpages  = {19},
  location  = {Munich, Germany}
}

@inproceedings{zhang15cvpr,
  author    = {Xucong Zhang and Yusuke Sugano and Mario Fritz and Bulling, Andreas},
  title     = {Appearance-based Gaze Estimation in the Wild},
  booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  pages     = {4511-4520}
}

% ETH-XGaze
@inproceedings{zhang2020ethxgaze,
  author = {Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and Siyu Tang and Otmar Hilliges},
  title = {ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation},
  year = {2020},
  booktitle = {European Conference on Computer Vision (ECCV)} 
} 

@misc{tobiiprofusion,
  author = {Tobii LTD},
  title = {Tobii Pro Fusion},
  year = 2019,
  url = {https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion},
  note = {Accessed on 2023-11-03},
}
@article{barzvi2020eyetrackingdigialreading,
  author  = {Bar-Zvi Shaked, Karin and Shamir, Adina and Vakil, Eli},
  journal = {Reading and Writing},
  number  = {8},
  title   = {An eye tracking study of digital text reading: a comparison between poor and typical readers},
  volume  = {33},
  year    = {2020},
  pages   = {1925 -- 1944},
  doi     = {10.1007/s11145-020-10021-9},
  url     = {https://doi.org/10.1007/s11145-020-10021-9}
}

@misc{bcs2022coc,
  author  = {BCS, The Chartered Institute for IT},
  title   = {Code of Conduct for BCS Members},
  version = {8},
  url     = {https://www.bcs.org/media/2211/bcs-code-of-conduct.pdf},
  urldate = {2023-11-12},
  year    = {2022}
}

@article{cheng2021survey,
  title   = {Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark},
  author  = {Yihua Cheng and Haofei Wang and Yiwei Bao and Feng Lu},
  journal = {arXiv preprint arXiv:2104.12668},
  year    = {2021}
}

@book{frisbe2022building,
  title     = {Building Browser Extensions: Create Modern Extensions for Chrome, Safari, Firefox, and Edge},
  author    = {Matt Frisbie},
  publisher = {Apress},
  isbn      = {148428724X,9781484287248},
  year      = {2022},
  edition   = {1},
  url       = {http://gen.lib.rus.ec/book/index.php?md5=7E63E6249A822FB3B8B5F6D63706F6E2}
}

% Gazecapture
@article{kar2017review,
  author  = {Kar, Anuradha and Corcoran, Peter},
  journal = {IEEE Access},
  title   = {A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms},
  year    = {2017},
  volume  = {5},
  number  = {},
  pages   = {16495-16519},
  doi     = {10.1109/ACCESS.2017.2735633}
}

@misc{krafka2016eye,
  title         = {Eye Tracking for Everyone},
  author        = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},
  year          = {2016},
  eprint        = {1606.05814},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{mturk,
  author  = {Amazon},
  title   = {Mechanical Turk},
  year    = {2005},
  url     = {https://www.mturk.com/},
  urldate = {2023-11-14}
}

@inproceedings{ohno2002freegaze,
  author    = {Ohno, Takehiko and Mukawa, Naoki and Yoshikawa, Atsushi},
  title     = {FreeGaze: A Gaze Tracking System for Everyday Gaze Interaction},
  year      = {2002},
  isbn      = {1581134673},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/507072.507098},
  doi       = {10.1145/507072.507098},
  abstract  = {In this paper we introduce a novel gaze tracking system called FreeGaze, which is designed for the use of everyday gaze interaction. Among various possible applications of gaze tracking system, Human-Computer Interaction (HCI) is one of the most promising elds. However, existing systems require complicated and burden-some calibration and are not robust to the measurement variations. To solve these problems, we introduce a geometric eyeball model and sophisticated image processing. Unlike existing systems, our system needs only two points for each individual calibration. When the personalization nishes, our system needs no more calibration before each measurement session. Evaluation tests show that the system is accurate and applicable to everyday use for the applications.},
  booktitle = {Proceedings of the 2002 Symposium on Eye Tracking Research \& Applications},
  pages     = {125,132},
  numpages  = {8},
  keywords  = {gaze interaction, the eyeball model, FreeGaze, Gaze tracing system},
  location  = {New Orleans, Louisiana},
  series    = {ETRA '02}
}

% ETH-XGaze
@misc{palmero2018recurrent,
  title         = {Recurrent CNN for 3D Gaze Estimation using Appearance and Shape Cues},
  author        = {Cristina Palmero and Javier Selva and Mohammad Ali Bagheri and Sergio Escalera},
  year          = {2018},
  eprint        = {1805.03064},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
} 

@misc{plasmo,
  author       = {Plasmo},
  howpublished = {Github},
  title        = {Plasmo Browser Extension Framework },
  year         = {2022}
}


@article{rakhmatulin2020deep,
  title    = {Deep Neural Networks for Low-Cost Eye Tracking},
  journal  = {Procedia Computer Science},
  volume   = {176},
  pages    = {685-694},
  year     = {2020},
  note     = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2020.09.041},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050920319360},
  author   = {Ildar Rakhmatulin and Andrew T. Duchowski},
  keywords = {eye tracking, deep learning in eye tracking, yolov3 in eye tracking, deep learning in gaze tracking},
  abstract = {The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center).}
}

@article{seonwook2019fewshot,
  author     = {Seonwook Park and
                Shalini De Mello and
                Pavlo Molchanov and
                Umar Iqbal and
                Otmar Hilliges and
                Jan Kautz},
  title      = {Few-shot Adaptive Gaze Estimation},
  journal    = {CoRR},
  volume     = {abs/1905.01941},
  year       = {2019},
  url        = {http://arxiv.org/abs/1905.01941},
  eprinttype = {arXiv},
  eprint     = {1905.01941},
  timestamp  = {Tue, 11 Jul 2023 08:22:03 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1905-01941.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{tobiiprofusion,
  author = {Tobii LTD},
  title  = {Tobii Pro Fusion},
  year   = 2019,
  url    = {https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion},
  note   = {Accessed on 2023-11-03}
}

@article{valliappan2020accelerating,
  author   = {Valliappan, Nachiappan
              and Dai, Na
              and Steinberg, Ethan
              and He, Junfeng
              and Rogers, Kantwon
              and Ramachandran, Venky
              and Xu, Pingmei
              and Shojaeizadeh, Mina
              and Guo, Li
              and Kohlhoff, Kai
              and Navalpakkam, Vidhya},
  title    = {Accelerating eye movement research via accurate and affordable smartphone eye tracking},
  journal  = {Nature Communications},
  year     = {2020},
  month    = {Sep},
  day      = {11},
  volume   = {11},
  number   = {1},
  pages    = {4553},
  abstract = {Eye tracking has been widely used for decades in vision research, language and usability. However, most prior research has focused on large desktop displays using specialized eye trackers that are expensive and cannot scale. Little is known about eye movement behavior on phones, despite their pervasiveness and large amount of time spent. We leverage machine learning to demonstrate accurate smartphone-based eye tracking without any additional hardware. We show that the accuracy of our method is comparable to state-of-the-art mobile eye trackers that are 100x more expensive. Using data from over 100 opted-in users, we replicate key findings from previous eye movement research on oculomotor tasks and saliency analyses during natural image viewing. In addition, we demonstrate the utility of smartphone-based gaze for detecting reading comprehension difficulty. Our results show the potential for scaling eye movement research by orders-of-magnitude to thousands of participants (with explicit consent), enabling advances in vision research, accessibility and healthcare.},
  issn     = {2041-1723},
  doi      = {10.1038/s41467-020-18360-5},
  url      = {https://doi.org/10.1038/s41467-020-18360-5}
}

@article{yiuming2015eyegazetracking,
  author  = {Cheung, Yiu-ming and Peng, Qinmu},
  journal = {IEEE Transactions on Human-Machine Systems},
  title   = {Eye Gaze Tracking With a Web Camera in a Desktop Environment},
  year    = {2015},
  volume  = {45},
  number  = {4},
  pages   = {419-430},
  doi     = {10.1109/THMS.2015.2400442}
}

@inproceedings{yu2019deep,
  author    = {Yu, Yu and Liu, Gang and Odobez, Jean-Marc},
  title     = {Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model},
  year      = {2019},
  isbn      = {978-3-030-11011-6},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  url       = {https://doi.org/10.1007/978-3-030-11012-3_35},
  doi       = {10.1007/978-3-030-11012-3_35},
  abstract  = {As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones.},
  booktitle = {Computer Vision - ECCV 2018 Workshops: Munich, Germany, September 8-14, 2018, Proceedings, Part II},
  pages     = {456-474},
  numpages  = {19},
  location  = {Munich, Germany}
}

@inproceedings{zhang15cvpr,
  author    = {Xucong Zhang and Yusuke Sugano and Mario Fritz and Bulling, Andreas},
  title     = {Appearance-based Gaze Estimation in the Wild},
  booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  pages     = {4511-4520}
}

@inproceedings{zhang2020ethxgaze,
  author    = {Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and Siyu Tang and Otmar Hilliges},
  title     = {ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation},
  year      = {2020},
  booktitle = {European Conference on Computer Vision (ECCV)}
}